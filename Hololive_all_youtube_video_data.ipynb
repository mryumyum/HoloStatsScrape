{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2280bd27-30df-4dbb-8607-81ae4b296e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Sample Python code for youtube.channels.list\n",
    "# See instructions for running these code samples locally:\n",
    "# https://developers.google.com/explorer-help/code-samples#python\n",
    "\n",
    "import os\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import isodate\n",
    "import pickle\n",
    "import datetime\n",
    "import pytz\n",
    "from dateutil import tz\n",
    "from IPython.display import JSON\n",
    "from datetime import timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa2b35-f25c-48ef-8ab8-afbd573a2745",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NOT MY CODE ALL CREDIT TO THE BELOW FUNCTIONS GOES TO THU VU:\n",
    "https://www.youtube.com/watch?v=D56_Cx36oGY&t=456s\n",
    "\n",
    "https://www.youtube.com/@Thuvu5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7b746-6db9-4640-82db-e3129ea07a62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \n",
    "    all_data = []\n",
    "    request = youtube.channels().list(part=\"snippet,contentDetails,statistics\", id=','.join(channel_ids))\n",
    "    \n",
    "    #request2 = youtube.playlistItems().list(part=\"snippet,contentDetails\", maxResults=25, playlistId=\"UUgmPnx-EEeOrZSg5Tiw7ZRQ\")\n",
    "    \n",
    "    response = request.execute()\n",
    "    \n",
    "    # Pretty Print Response\n",
    "    #JSON(response)\n",
    "    \n",
    "    for item in response['items']:\n",
    "        data = {'channelName': item['snippet']['title'],\n",
    "                'subscribers': item['statistics']['subscriberCount'],\n",
    "                'views': item['statistics']['viewCount'],\n",
    "                'totalVideos': item['statistics']['videoCount'],\n",
    "                'playlistId': item['contentDetails']['relatedPlaylists']['uploads']\n",
    "               }\n",
    "        all_data.append(data)\n",
    "        \n",
    "    return(pd.DataFrame(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20212b3a-503f-40f5-b184-7fa4c8e2b4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gets the video ids of all videos on the channel *INCLUDING* lives\n",
    "def get_video_ids(youtube, playlist_id):\n",
    "    \n",
    "    video_ids = []\n",
    "    request = youtube.playlistItems().list(part=\"snippet,contentDetails\", maxResults=50, playlistId=playlist_id)\n",
    "    response = request.execute()\n",
    "    \n",
    "    for item in response['items']:\n",
    "        video_ids.append(item['contentDetails']['videoId'])\n",
    "    \n",
    "    # 50 is the max results per page on youtube, so we need to go to the next page\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    \n",
    "    while next_page_token is not None:\n",
    "        request = youtube.playlistItems().list(part=\"contentDetails\", maxResults=50, playlistId=playlist_id, pageToken=next_page_token)\n",
    "        response = request.execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581faa1-d4cc-43d5-98f0-90537ff31ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_video_details(youtube, video_ids):\n",
    "    \n",
    "    all_video_info = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(part=\"snippet,statistics,contentDetails\", id=','.join(video_ids[i:i+50]))\n",
    "        response = request.execute()\n",
    "\n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n",
    "                             'statistics': ['viewCount','likeCount', 'favoriteCount', 'commentCount'],\n",
    "                             'contentDetails':['duration', 'definition', 'caption']\n",
    "                            }\n",
    "            video_info = {}\n",
    "            video_info['video_id'] = video['id']\n",
    "\n",
    "            for k in stats_to_keep.keys():\n",
    "                for v in stats_to_keep[k]:\n",
    "                    try:\n",
    "                        video_info[v] = video[k][v]\n",
    "                    except:\n",
    "                        video_info[v] = None\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "        \n",
    "    return pd.DataFrame(all_video_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc535d2-e2b4-46fb-b704-f71bdb16041f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "channel_ids = ['UCoSrY_IQQVpmIRZ9Xf-y93g', 'UCCzUftO8KOVkV4wQG1vkUvg','UC1DCedRgGHBdm81E1llLhOQ','UCL_qhgtOy0dy1Agp8vkySQg',\n",
    "               'UCdn5BQ06XqgXoAxIhbqw5Rg','UCjLEmnpCNeisMxy134KPwWw','UChAnqc_AY5_I3Px5dig3X1Q','UC5CwaMl1eIgY8h02uZw7u8A',\n",
    "               'UC1opHUrw8rvnsadT-iGp7Cg','UC-hM6YJuNYVAmUWxeIr9FeA','UCyl1z3jo3XHR1riLFKG5UAg','UCdyqAaZDKHXg4Ahi7VENThQ',\n",
    "               'UCvaTdHTWBGv3MKj3KVqJVCw','UCMwGHR0BTZuLsmjY_NT5Pwg','UCvzGlP9oQwU--Y0r9id_jnA','UCHsx4Hqa-1ORjQTh9TYDhww',\n",
    "               'UC7fk0CB07ly8oSl0aqKkqFg','UCqm3BQLlJfvkTsX_hvm0UmA','UC1CfXB_kRs3C-zaeTG3oGyg','UCS9uQI-jC3DE0L4IpXyvr6w',\n",
    "               'UCQ0UDLQCjY0rmuxCDE38FGg','UCZlDXzGoo7d44bwdNObFacg','UCUKD-uaobj9jiqB-VXt71mA','UCP0BspO_AMEe3aQqqpo89Dg',\n",
    "               'UCYz_5n-uDuChHtLo7My1HnQ','UC1uv2Oq6kNxgATlCiez59hw','UCXTpFs_3PqI41qX2d9tL2Rw','UCFKOVgVbGmX65RxO3EtH3iw',\n",
    "               'UCK9V2B22uJYu3N7eR_BT9QA','UCIBY1ollUsauvVi4hW4cumw','UCp6993wxpyDPHUpavwDFqgg','UCp-5t9SrOQwXMU7iIjQfARg',\n",
    "               'UCAWSyEs_Io8MtpY3m-zqILA','UCENwRMx5Yh42zWpzURebzTw','UCvInZx9h3jC2JzsIzoOebWg','UC1suqwovbL1kzsoaZgFZLKg',\n",
    "               'UC6eWCld0KwmyHFbAqK3V-Rw','UC8rcEBzJSleTkf_-agPM20g','UCa9Y57gfeY0Zro_noHRVrnw','UCDqI2jOz0weumE8s7paEk6g',\n",
    "               'UCmbs8T6MWqUHP1tIQvSgKrg','UC3n5uGu18FoCy23ggWWp8tA','UCD8HOxPs4Xvsm8H0ZxXGiBw','UC0TXe_LYZ4scaW2XMyi5_kw',\n",
    "               'UCOyYb1c43VlX9rc_lT6NKQw','UCFTLzh12_nrtzqBPsTCqenA','UCTvHWSfBZgtxE4sILOaurIQ','UC_vMYWcDjmfdpH6r4TTn1MQ',\n",
    "               'UChgTyjG-pdNvxxhdsXfHQ5Q','UCs9_O1tRPMQTHQ-N_L6FU2g','UCO_aKKYxn4tvrqPjcTzZ6EQ','UCgmPnx-EEeOrZSg5Tiw7ZRQ',\n",
    "               'UCAoy6rzhSf4ydcYjJw3WoVg','UCZLZ8Jjx_RN2CXloOmgTHVg','UC727SQYUvx5pDDGQpTICNWg','UCsUj0dszADCGbF3gNrQEuSQ',\n",
    "               'UCyxtGMdWlURZ30WSnEjDOQw','UCDRWSO281bIHYVi-OV3iFYA','UC2hx0xVkMoHGWijwr_lA01w','UC7MMNHR-kf9EN1rXiesMTMw',\n",
    "               'UC7gxU6NXjKF1LrgOddPzgTw','UCHP4f7G2dWD4qib7BMatGAw','UC060r4zABV18vcahAWR1n7w','UCMqGG8BRAiI1lJfKOpETM_w']\n",
    "\n",
    "api_key = ''\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "youtube = googleapiclient.discovery.build(api_service_name, api_version, developerKey=api_key)\n",
    "\n",
    "#channel_id = ['UCgmPnx-EEeOrZSg5Tiw7ZRQ']\n",
    "all_channel_data = pd.DataFrame(columns=['channelName', 'subscribers', 'views', 'totalVideos', 'playlistId'])\n",
    "all_channel_vids = pd.DataFrame(columns=['video_id','channelTitle','title','description','tags','publishedAt','viewCount','likeCount','favoriteCount','commentCount','duration','definition','caption'])\n",
    "\n",
    "for chan_id in channel_ids:\n",
    "    \n",
    "    #print('Channel ID: ' + f'{chan_id}')\n",
    "    \n",
    "    channel_id = []\n",
    "    channel_id.append(chan_id)\n",
    "    channel_stats = get_channel_stats(youtube, channel_id)\n",
    "    all_channel_data = pd.concat([all_channel_data, channel_stats])\n",
    "    playlist_id = channel_stats.iloc[0][-1]\n",
    "    \n",
    "    #print('Playlist ID: ' + f'{playlist_id}'+'\\n')\n",
    "    \n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "    video_df = get_video_details(youtube, video_ids)\n",
    "    \n",
    "    all_channel_vids= pd.concat([all_channel_vids, video_df])\n",
    "\n",
    "all_channel_vids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f739dd-11af-42f6-ac86-08ecb5c4688f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_channel_vids.to_excel(\"Hololive_channel_video_data.xlsx\")\n",
    "    \n",
    "all_channel_data = pd.DataFrame(all_channel_data)\n",
    "all_channel_data.to_excel(\"Hololive_channel_stats.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a549061-341a-48cf-a262-b3c9d158ecdf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The YouTube LiveStreams API requires OAuth 2.0 Authorization for the stream start time, so we have to calculate it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2f487-15c1-4e38-85cd-862156d4bbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c819874-7693-4adb-a988-7f2ef5ca368a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= pd.read_excel('Hololive_channel_video_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f75e41-1507-4c3b-bee1-b5c532cf6f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['duration'] = df['duration'].apply(isodate.parse_duration)\n",
    "df['duration'] = df['duration']/np.timedelta64(1, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8bd257-69ca-43d6-9e10-e24f61a0658a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['publishedAt'] = pd.to_datetime(df['publishedAt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe206e8-8422-402c-a9ce-a2f2a39afc45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stream_end_times = pd.to_datetime(df['publishedAt'], format='%H:%M').dt.time\n",
    "stream_duration = pd.to_datetime(df['duration'], unit='s').dt.time\n",
    "\n",
    "stream_end_seconds = []\n",
    "stream_duration_seconds = []\n",
    "\n",
    "for x in stream_end_times.index:\n",
    "    \n",
    "    time = str(stream_end_times[x])\n",
    "    date_time = datetime.datetime.strptime(time, \"%H:%M:%S\")\n",
    "    a_timedelta = date_time - datetime.datetime(1900, 1, 1)\n",
    "    seconds = a_timedelta.total_seconds()\n",
    "    stream_end_seconds.append(seconds)\n",
    "    \n",
    "print(len(stream_end_seconds))\n",
    "\n",
    "stream_duration_seconds = df['duration'].to_numpy()\n",
    "    \n",
    "print(len(stream_duration_seconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15c61f-30c6-4ed7-a6e9-4b2b590d668f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Before I relized that you could subtract time from timestamps I implemented a method that took the stream end time and subtracted that from the duration (both in seconds) and then converted the hours to UST\n",
    "#####  The First problem that arose is what if the duration was longer than the end time? we have to account for people streaming over midnight which is what the else statement does below\n",
    "#####  The next problem (that made me find out that you could subtract time from timestamps) made me realize that this method would not be able to take into account the day of the week as the below conversion only returns the time and not the date time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b78f9-5161-4b4d-880c-eb273dfe82e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "##############################################################################################################################\n",
    "# DO\n",
    "# NOT\n",
    "# RUN\n",
    "# THIS\n",
    "# CELL\n",
    "##############################################################################################################################\n",
    "\n",
    "stream_start_time = []\n",
    "\n",
    "for x in range(len(stream_duration_seconds)):\n",
    "    \n",
    "    if int(stream_duration_seconds[x]) < int(stream_end_seconds[x]):    \n",
    "        stream_start_time.append(int(stream_end_seconds[x]) - int(stream_duration_seconds[x]))\n",
    "        \n",
    "    else:\n",
    "        sd = int(stream_duration_seconds[x]) - int(stream_end_seconds[x])\n",
    "        app = 86400 - sd\n",
    "        stream_start_time.append(app)\n",
    "\n",
    "df2 =pd.DataFrame()\n",
    "sst = pd.DataFrame(stream_start_time)\n",
    "#df = df.drop(columns = ['Stream Start Time UST'])\n",
    "df2.insert(7, 'Stream Start Time UST', pd.to_datetime(sst[0], unit='s').dt.time)\n",
    "stream_start_time_ust = pd.to_datetime(sst[0], unit='s')\n",
    "\n",
    "\n",
    "#stream_start_time_cst = stream_start_time_ust.astimezone(pytz.timezone('US/Central')).strftime('%Y-%m-%d %H:%M:%S %Z%z')\n",
    "#stream_start_time_ust.dt.time_UTC.tz_localize('UTC').tz_convert('US/Central')\n",
    "#stream_start_time_ust = datetime.date().astimezone(pytz.timezone('US/Eastern')).strftime('%y/%m/%d %H:%M:%S')\n",
    "\n",
    "stream_start_time_cst = []\n",
    "stream_start_time_jst = []\n",
    "\n",
    "for x in stream_start_time_ust.index:\n",
    "    stream_start_time_cst.append(stream_start_time_ust[x].tz_localize('UTC').tz_convert('US/Central'))\n",
    "\n",
    "for x in stream_start_time_ust.index:\n",
    "    stream_start_time_jst.append(stream_start_time_ust[x].tz_localize('UTC').tz_convert('Japan'))\n",
    "\n",
    "\n",
    "sstc = pd.DataFrame(stream_start_time_cst)\n",
    "cst_dm = pd.to_datetime(sstc[0], unit='s', utc = False)\n",
    "df2.insert(8, 'Day of Week CST', cst_dm.dt.day_name())\n",
    "df2.insert(8, 'Month CST', cst_dm.dt.month_name())\n",
    "stream_start_time_cst = pd.to_datetime(sstc[0], unit='s', utc = False).dt.time\n",
    "\n",
    "sstj = pd.DataFrame(stream_start_time_jst)\n",
    "jst_dm = pd.to_datetime(sstj[0], unit='s', utc = False)\n",
    "df2.insert(8, 'Day of Week JST', jst_dm.dt.day_name())\n",
    "df2.insert(8, 'Month JST', jst_dm.dt.month_name())\n",
    "stream_start_time_jst = pd.to_datetime(sstj[0], unit='s', utc = False).dt.time\n",
    "\n",
    "#df2 = df2.drop(columns = ['Stream Start Time CST'])\n",
    "#df2 = df2.drop(columns = ['Stream Start Time JST'])\n",
    "df2.insert(7, 'Stream Start Time JST', stream_start_time_jst)\n",
    "df2.insert(7, 'Stream Start Time CST', stream_start_time_cst)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab3383-e98f-4b6f-9aa7-84313509f4d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### The below method is able to take into account time zones as well as the different days of the week in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a056d4c-252a-4f3b-9eaf-93586bb7e37d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sst = pd.DataFrame(stream_duration_seconds)\n",
    "\n",
    "stream_start_UST = []\n",
    "stream_start_CST = []\n",
    "stream_start_PST = []\n",
    "stream_start_JST = []\n",
    "\n",
    "\n",
    "for x in sst.index:\n",
    "    duration_time = pd.to_datetime(sst.loc[x], unit='s').dt.time\n",
    "    hms = duration_time[0].strftime(\"%H:%M:%S\").split(':')\n",
    "    hours = int(hms[0])\n",
    "    minutes = int(hms[1])\n",
    "    seconds = int(hms[2])\n",
    "\n",
    "    UST_start = df['publishedAt'][x] - timedelta(hours=hours, minutes=minutes, seconds=seconds)\n",
    "    stream_start_UST.append(UST_start)\n",
    "    stream_start_CST.append(UST_start.tz_convert('US/Central'))\n",
    "    stream_start_PST.append(UST_start.tz_convert('US/Pacific'))\n",
    "    stream_start_JST.append(UST_start.tz_convert('Japan'))\n",
    "\n",
    "df.insert(7, 'Stream Start Time PST', stream_start_PST)\n",
    "\n",
    "df.insert(7, 'Stream Start Time CST', stream_start_CST)\n",
    "df.insert(7, 'Day of Week CST', df['Stream Start Time CST'].dt.day_name())\n",
    "df.insert(7, 'Month CST', df['Stream Start Time CST'].dt.month_name())\n",
    "\n",
    "df.insert(7, 'Stream Start Time JST', stream_start_JST)\n",
    "df.insert(7, 'Day of Week JST', df['Stream Start Time JST'].dt.day_name())\n",
    "df.insert(7, 'Month JST', df['Stream Start Time JST'].dt.month_name())\n",
    "\n",
    "df.insert(7, 'Stream Start Time UST', stream_start_UST)\n",
    "df.insert(7, 'Day of Week UST', df['Stream Start Time UST'].dt.day_name())\n",
    "df.insert(7, 'Month UST', df['Stream Start Time UST'].dt.month_name())\n",
    "df\n",
    "\n",
    "#sst.tz_localize('UTC').tz_convert('US/Central')\n",
    "\n",
    "##given_time = datetime.strptime(df['publishedAt'][0], '%d/%m/%Y %H:%M:%S.%f')\n",
    "#stream_start_time_ust = pd.to_datetime(sst[0], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfcdd1f-d76b-407c-93a8-c459cd7f20f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['publishedAt'] = df['publishedAt'].dt.tz_localize(None)\n",
    "df['Stream Start Time CST'] = df['Stream Start Time CST'].dt.tz_localize(None)\n",
    "df['Stream Start Time JST'] = df['Stream Start Time JST'].dt.tz_localize(None)\n",
    "df['Stream Start Time PST'] = df['Stream Start Time PST'].dt.tz_localize(None)\n",
    "df['Stream Start Time UST'] = df['Stream Start Time UST'].dt.tz_localize(None)\n",
    "df.to_excel(\"Hololive_channel_video_data_REF.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
